{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "collapsed": true,
        "id": "TdXNFfShbxgj",
        "outputId": "03da817f-18ca-4e5d-c225-3ae3e1a81665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install langchain\\n!pip install langchain-community\\n!pip install tiktoken\\n!pip install chromadb\\n!pip install python-dotenv\\n!pip install langchain-google-genai\\n!pip install pdfminer.six'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# remove comment before running\n",
        "\"\"\"\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install tiktoken\n",
        "!pip install chromadb\n",
        "!pip install python-dotenv\n",
        "!pip install langchain-google-genai\n",
        "!pip install pdfminer.six\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you get api key error please create file named .env and add your api keys with following names\n",
        "# GOOGLE_API_KEY\n",
        "# ACCESS_TOKEN\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "import os\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "access_token = os.getenv(\"ACCESS_TOKEN\")\n",
        "# Set the environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "os.environ[\"ACCESS_TOKEN\"] = access_token"
      ],
      "metadata": {
        "id": "lpp5sidafiHY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load PDF"
      ],
      "metadata": {
        "id": "Yn8wEAJieqKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader"
      ],
      "metadata": {
        "id": "PgMfOPkccsCE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(path):\n",
        "  pdf_loader = PDFMinerLoader(path)\n",
        "  docs = pdf_loader.load()\n",
        "  return docs"
      ],
      "metadata": {
        "id": "vG8gxUTpc-TY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split PDF into Chunks"
      ],
      "metadata": {
        "id": "vYpLdum2ewye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "-01c0GtKdtcr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(docs, chunk_size=2000, chunk_overlap=100):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  chunks = text_splitter.split_documents(docs)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "njRf1y62dfT0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Embedings and Store into ChormaDB"
      ],
      "metadata": {
        "id": "XYEgxnzSe0lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "cPpOyOlvebCh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embd_and_store(chunks):\n",
        "  embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "  vector_store = Chroma.from_documents(chunks,embd,persist_directory=\"./chroma\")\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "QAb_oSyXfBq6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = load_pdf(\"./tesi.pdf\") # Add path to your pdf\n",
        "chunks = split_text(docs)\n",
        "vector_store = embd_and_store(chunks)"
      ],
      "metadata": {
        "id": "NPagWdmnfPw6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Embeddings\n"
      ],
      "metadata": {
        "id": "i6ZcCROGpyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embd(query):\n",
        "  embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "  return embd.embed_query(query)"
      ],
      "metadata": {
        "id": "dUD1a6YriZtL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' query = \"What does the document say about AI?\"\n",
        "retrieved_docs = vector_store.similarity_search_with_score(query=query, k=5)\n",
        "\n",
        "# Display results with distances\n",
        "for i, (doc, score) in enumerate(retrieved_docs, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Text: {doc.page_content}\")\n",
        "    print(f\"Distance Score: {score}\\n\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "collapsed": true,
        "id": "ETAcwhMoiwL0",
        "outputId": "1345bf53-5714-44d1-95a6-e94a6353b331"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' query = \"What does the document say about AI?\"\\nretrieved_docs = vector_store.similarity_search_with_score(query=query, k=5)\\n\\n# Display results with distances\\nfor i, (doc, score) in enumerate(retrieved_docs, 1):\\n    print(f\"Result {i}:\")\\n    print(f\"Text: {doc.page_content}\")\\n    print(f\"Distance Score: {score}\\n\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Model"
      ],
      "metadata": {
        "id": "g45LC1nDp3wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM,AutoTokenizer"
      ],
      "metadata": {
        "id": "9YQy1iUElbj_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
      ],
      "metadata": {
        "id": "9pniHRkYlYbY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OJe3EAnlXtV",
        "outputId": "801fdf87-63b9-41a6-b2e8-d0a8f4de8eed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query):\n",
        "    retrived_doc = vector_store.similarity_search(query=query,k=1)\n",
        "    context = \"\"\n",
        "    for doc in retrived_doc:\n",
        "        context += doc.page_content + \"\\n\"\n",
        "    prompt = f\"\"\"\n",
        "[System]: You are a helpful AI assistant. Provide concise and accurate answers to user questions.based on the context you recoved form the document provided.\n",
        "[Context]: {context}\n",
        "[User]: {query}\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    output = model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,  # Pass the attention mask\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    # max_length=100,  # Limit the response length\n",
        "    temperature=0.7,  # Balances creativity and determinism\n",
        "    top_p=0.9,       # Nucleus sampling: keeps the top 90% of probability mass\n",
        "    top_k=50,        # Limits sampling to the top 50 tokens\n",
        "    num_beams=1,     # Beam search for better coherence\n",
        "    do_sample=True,  # Enables sampling for more diverse outputs\n",
        "    repetition_penalty=1.2,  # Reduces repetition in the output\n",
        "    no_repeat_ngram_size=2,  # Prevents repeating 2-grams\n",
        "    early_stopping=True,     # Stops generation when the model is confident\n",
        ")\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "eDE9fEKljle9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is similarity metrics\"\n",
        "response = get_response(query)"
      ],
      "metadata": {
        "id": "aA-Wi_OzoPHA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiFaTaeNoDZC",
        "outputId": "c3f6f7c1-57ce-41da-8eea-d6fc2d48808a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[System]: You are a helpful AI assistant. Provide concise and accurate answers to user questions.based on the context you recoved form the document provided.\n",
            "[Context]: using the formula:\n",
            "\n",
            "Cosine Similarity =\n",
            "\n",
            "A · B\n",
            "∥A∥∥B∥\n",
            "\n",
            "where A and B are the vectors being compared, A · B is the dot product, and\n",
            "∥A∥ and ∥B∥ are their norms. This metric is useful for assessing how aligned\n",
            "two vectors are in the same direction, regardless of their magnitude, and is\n",
            "often used to measure semantic similarity between words or documents.\n",
            "\n",
            "9\n",
            "\fRelated Work\n",
            "\n",
            "• Euclidean distance measures the linear distance between two vectors in\n",
            "\n",
            "multidimensional space and is calculated using the formula:\n",
            "\n",
            "Euclidean Distance =\n",
            "\n",
            "ö\n",
            "õ\n",
            "õ\n",
            "ô\n",
            "\n",
            "n\n",
            "Ø\n",
            "\n",
            "i=1\n",
            "\n",
            "(ai − bi)2\n",
            "\n",
            "where ai and bi are the component values of vectors A and B, and n is\n",
            "the number of dimensions in the vector. This metric provides an absolute\n",
            "measure of the distance between the two vectors, useful for identifying overall\n",
            "differences in their representations.\n",
            "\n",
            "• maximum inner product is calculated as:\n",
            "\n",
            "Inner Product = A · B =\n",
            "\n",
            "n\n",
            "Ø\n",
            "\n",
            "i=1\n",
            "\n",
            "ai · bi\n",
            "\n",
            "where ai and bi are the components of vectors A and B. This metric directly\n",
            "considers the dot product, measuring similarity in terms of alignment and\n",
            "intensity, and is particularly useful in contexts like recommendation systems,\n",
            "where both the direction and magnitude of the vectors are important.\n",
            "\n",
            "2.3 LLM\n",
            "\n",
            "Large Language Models (LLMs) represent one of the most significant innovations\n",
            "in the field of artificial intelligence and natural language processing (NLP), with\n",
            "the ability to understand, generate and even interact in human language with a\n",
            "good level of satisfaction. These models are the result of years of research and\n",
            "development, culminating in the creation of advanced architectures that exploit\n",
            "billions of parameters and vast text datasets to learn the complexity and richness\n",
            "of natural language. In training LLMs, models are exposed to huge amounts of\n",
            "unlabelled text and learn to predict the next token in a sequence, thus gaining a\n",
            "deep understanding of linguistic structures and semantic relationships.\n",
            "\n",
            "[User]: what is similarity metrics\n",
            "</think>\n",
            "\n",
            "Similarity Metrics**  \n",
            "The concept of **similarity metrics** involves quantifying how alike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YFSMAXCp_Zq"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}